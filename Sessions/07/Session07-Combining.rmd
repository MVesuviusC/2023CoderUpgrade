---
title: "SCRGOT 2023 Coder Upgrade Session 07 - Combining single cell datasets"
author: "Ryan Roberts"
date: "4/16/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(patchwork)
library(rrrSingleCellUtils)
library(Seurat)
library(parallel)

theme_set(theme_classic())
```

## Introduction to the datasets

Within the accompanying "Datasets" folder, you will find two RData files, each containing a collection of single cell datasets. I have taken care of a lot of pre-processing for you, so that you can focus during this workshop on integrating the datasets. 

The "patient_tumors.RData" file contains two data objects, each of these in turn containing a list of Seurat objects. These data originated from scRNA-seq performed directly on primary bone tumors resected from patients, then processed to produce single cell libraries and sequenced (10X 3' workflow). These data were downloaded from the GEO database (GSE162454). Each element of each list contains cells from the same tumor. The list named "pt_tumor" contains subsets of the tumor cells from each sample and the "pt_stroma" list contains subsets of the stromal cells identified in same samples (I already took care of all that for you). The names of each element in the list (OS_1, OS_2, etc) are the patient IDs reported in the publication. Note that each of these samples was collected from a different patient on a different day, sometimes in a different lab, and processed separately from all of the others. All of the cells in both data objects are human.

The "xeno_tumors.RData" file contains two similarly-structured lists of Seurat objects from a mouse xenograft experiment, where immunodeficient mice were injected by tail vein with human bone tumor cells to produce metastatic lung tumors. Lungs were then harvested at several timepoints and processed on the day of harvest to produce single cell libraries that were sequenced. These samples each contain the same tumor cells, harvested at different stages along the path toward metastatic colonization, but also contain the surrounding lung cells (isolated using a niche labeling technique). Importantly, the "xeno_tumors" object is a list containing human tumor cells, while the "xeno_stroma" object contains murine stromal cells--the sequenced libraries were aligned to a mixed human/mouse genome.

Start by practicing your skills learned yesterday in the Intro session to take one Seurat object within the "pt_tumor" list to a UMAP plot. HINT: Read10x,  CreateSeuratObject, PercentageFeatureSet, VlnPlot, subset are all done. You'll need to run NormalizeData, FindVariableFeatures, ScaleData, RunPCA, FindNeighbors, FindClusters, RunUMAP, DimPlot.

```{r}
load("Datasets/patient_tumors.RData")

t1 <- pt_tumor[[5]] %>%
    NormalizeData() %>%
    FindVariableFeatures() %>%
    ScaleData() %>%
    RunPCA() %>%
    FindNeighbors() %>%
    FindClusters() %>%
    RunUMAP(dims = 1:30)

DimPlot(t1, pt.size = 1, label = TRUE) +
    ggtitle("Sample OS_5 - initial clustering") +
    coord_fixed()
```

# Merging datasets

The first step toward combining multiple datasets is to move all of the normalized data (from all of the objects that you want to combine) into a single Seurat object. This is best accomplished by creating a list of individual Seurat objects (which I've already done for you), then using the merge command to combine them into one. 

You need to have a little forethought in the way that you do this. If you don't create metadata variables that will track the origins of each cell, when the cells are all combined together, you won't be able to tell which cells came from which samples.

Step-by-step instructions:
1. (I already did this for you) Create a list containing individual Seurat objects representing each of the datasets in the "PatientTumors" folder. You have three options for how you might approach this:
    - Acceptable: create each of the Seurat objects separately, then combine them into a list using the list() command.
    - Better: create a for() loop that loops through code to create the Seurat objects, adding them to the list object without an intermediate (the individual Seurat object).
    - Best: create a dataframe, tibble, or list of vectors containing all of the variables that you will need to create the list of Seurat objects and a function to create a normalized Seurat object and add metadata, then use lapply to map the dataframe to the function, building the list of Seurat objects in a single step. (This is the most computationally efficient and flexible way to do it, and you can reuse the function you build over and over).
2. (Start here with the "pt_tumor" data set) Merge these into a single Seurat object using the merge() command. Candidly, this is a super awkward function, so I'll lay out for you the way it should look:
    
    combined_object <- merge(seurat_list[[1]], seurat_list[2:length(seurat_list)], add.cell.ids = sample_list)

The first element of the merge() is the first object that you want to combine, The second is "the rest" of the elements that you want to combine, and the add.cell.ids is a prefix added to the cell barcodes, in case some of them are the same from one sample to another (they won't get mistakenly put together or crash the operation because they're not unique if you use this option).
3. Perform all the usual operations to process your dataset, just like you did for the individual Seurat objects--FindVariableFeatures, ScaleData, yada yada yada through RunUMAP.
4. Now plot the data to see what you have. This is an important decision point. Based on the nature of the samples/dataset, the details surrounding how they were created, and the scientific question you intend to ask, you need to decide whether or not this will work without alignment, or whether it needs to be aligned (and what you'll use to attempt alignment/batch correction, if you do choose to use it). To check out how well aligned (or not) your data is, use the group.by and split.by options of DimPlot to use that metadata element you added earlier to either color by source or panel by source, respectively.
5. Are your samples clustering by sample or by cell type or a little of both?
6. In this example, we have tumor cells that seem very different one from the next, but stromal cells that might be similar. Repeat this process with the "pt_stroma" Seurat objects and compare results.

```{r}
# Build a list of samples to import
sample_list <- c("0Weeks", "2Weeks", "3Weeks", "5Weeks")

# Write a function to import a sample (be sure to label where it came from)
create_seurat <- function(x) {
    message(paste("Loading", x, "..."))
    s <- tenx_load_qc(paste0("Datasets/XenoTumors/", x), species_pattern = "^mm10-") 
    s$src <- x
    message(paste("Sample", x, "is loaded."))
    return(s)
}

# Map the sample_list to the create_seurat function with lapply to generate
# a list of Seurat objects
stroma <- mclapply(sample_list, create_seurat, mc.cores = 4L)
names(stroma) <- sample_list

violins <- list()
for(t in names(stroma)) {
    violins[[t]] <- VlnPlot(stroma[[t]],
        features = c("nFeature_RNA", "nCount_RNA", "percent.mt")) +
        plot_annotation(title = t,
            theme = theme(plot.title = element_text(size = 16)))
}

wrap_plots(violins, ncol = 2)

qc <- data.frame(
    sample = sample_list,
    ncount = c(20000, 6500, 25000, 25000),
    mtpercent = c(13, 13, 13, 13)
)

patient_tumor <- mclapply(seq_along(stroma), function(i) {
    x <- stroma[[i]] %>%
        subset(subset = nCount_RNA < qc$ncount[i] &
            percent.mt < qc$mtpercent[i]) %>%
        NormalizeData() %>%
        FindVariableFeatures() %>%
        ScaleData() %>%
        RunPCA() %>%
        FindNeighbors() %>%
        FindClusters() %>%
        RunUMAP(dims = 1:30)
    return(x)
})
names(patient_tumor) <- names(stroma)

dsample <- 500
stroma <- lapply(patient_tumor, function(x) {
    s <- x
    cell_list <- colnames(s)
    samp_number <- min(dsample, length(cell_list))
    cell_list <- sample(cell_list, samp_number)
    s <- subset(s, cells = cell_list)
    return(s)
})





```

















































*NOTE: The procedures we will use in this exercise will only work if R is running in a Mac/UNIX/Linux environment. This is because the most efficient (and embarrasingly easy to implement) solutions for multi-core computing in R rely on a process called "forking", which Windows doesn't support. You're OK if you're working on a Windows machine to remote into the cluster, or if you're using a local windows machine to pass commands to a remote Linux machine (like in VSCode). If you need to run multi-core on a Windows machine, check out solutions like the doParallel package.

## Examine the data

The "Datasets" folder contains several single cell datasets downloaded from two different GEO entries. The names of the next level folders (GSEXXXXXX) are the GEO accession IDs. Within these are a series of folders named by the sample ID reported in the publication, each containing the three barcodes/features/matrix files produced by a cellranger alignment. The tibble created below consolidates key data elements associated with each sample.

The samples used to create these datasets were tumors taken from individuals diagnosed with a type of bone tumor (osteosarcoma). Osteosarcoma has several different histologic subtypes (the "path" element below). Primary tumors form in bones, while the lungs are the most common site of metastasis (the "type" argument). There was no additional processing/sorting of the tumor samples before sequencing, so the datasets contain both tumor cells and the surrounding stromal and immune components. The "nCount" column contains a simple pre-determined qc cutoff to reduce doublets.

Within these data are opportunites to address several different scientific questions. Think of some question that you might ask of such data, then select the datasets that you'd use to answer it (for this exercise, ideally 6-10 datasets). Delete or comment out the rows that you won't use, then run the code to create the tibble.

```{r define_data}
geo_data <- tribble(
    ~gse, ~id, ~nCount, ~path, ~type,
    "GSE152048", "BC5", 18000, "Conventional", "Primary",
    "GSE152048", "BC6", 25000, "Conventional", "Primary",
    # "GSE152048", "BC10", 25000, "Conventional", "Lung Met",
    "GSE152048", "BC11", 30000, "Conventional", "Primary",
    "GSE152048", "BC16", 70000, "Conventional", "Primary",
    # "GSE152048", "BC17", 40000, "Chondroblastic", "Lung Met",
    # "GSE152048", "BC20", 70000, "Chondroblastic", "Primary",
    # "GSE152048", "BC21", 50000, "Intraosseous", "Primary",
    # "GSE152048", "BC22", 50000, "Chondroblastic", "Primary",
    "GSE162454", "OS_1", 50000, "Conventional", "Primary",
    "GSE162454", "OS_2", 45000, "Conventional", "Primary",
    "GSE162454", "OS_3", 23000, "Conventional", "Primary",
    "GSE162454", "OS_4", 50000, "Conventional", "Primary",
    "GSE162454", "OS_5", 50000, "Conventional", "Primary",
    "GSE162454", "OS_6", 45000, "Conventional", "Primary"
)
```

## Practice 1: Simple parallelization with mclapply (from the parallel package)

We will create a list of Seurat objects containing your chosen datasets. We will start by loading and processing the objects 

1. Create a function that will utilize the variables from the tibble you created to convert the data matrices found in the "Datasets" folder into Seurat objects, subsetting for nCount_RNA < $path as a super basic QC. HINT: feel free to use the tenx_load_qc function from rrrSingleCellUtils to streamline the process.
2. Now map the data from the tibble into the function using lapply to create a list of Seurat objects.
3. Then wrap this code with a pair of system timestamps [Sys.time()] and calculate the difference to document the time it takes to perform the operation.
4. Once you have this working, copy that block of code and paste it at the end of the chunk. Then, simply change the "lapply" command to "mclapply" and add the mc.cores argument, with the number of cores set to those you requested in your session.
5. Determine how much time is saved by running the processes in parallel.
6. BONUS. Want to see how all of this is happening by monitoring the processes running on the cluster in real time? Open a separate shell running on the same node and run "top". Then, set the process in motion and observe how your code utilizes the different cores.
7. Now repeat this procedure to create a function that will process your Seurat objects (NormalizeData, FindVariableFeatures, ScaleData, RunPCA, FindNeighbors, FindClusters, RunUMAP), then plot the UMAPs. How much time do you save with this step by running it parallel?
8. BONUS. Restructure your code so that the data loading and processing operations occur in a single parallel computing operation, rather than two separate parallel computing operations. Embed a series of timestamps to benchmark the two approaches and compare the results. Does combining the two operations increase efficiency? Why or why not?
9. BONUS. Are the data objects created using the serial and the parallel processing approaches identical? Why or why not? (see https://pat-s.me/reproducibility-when-going-parallel/ for some helpful information)

```{r parallel-1}
# Create a function to process matrices to a Seurat object
create_seurat <- function(x) {
    x <- tenx_load_qc(
        path_10x = paste0(
            "Datasets/",
            x$gse, "/",
            x$id),
        violin_plot = FALSE,
        sample_name = x$id
    )
    return(x)
}

# Split the tibble into a list of vectors
geo_data_2 <- geo_data %>%
    as.data.frame() %>%
    split(1:nrow(geo_data)) %>%
    `names<-`(geo_data$id)

# Set the start time
message("Starting serial processing for loading/creating Seurat objects...")
t <- c(Sys.time())

# Map the function onto the list of vectors
tumors_serial <- lapply(geo_data_2, create_seurat)

# Set the completion time for above and start time for below
message("Moving on to parallel processing for loading/creating Seurat objects...")
t[2] <- Sys.time()

# Now repeat the function using mclapply
tumors_parallel <- mclapply(geo_data_2, create_seurat, mc.cores = 10L)

# Mark completion time for the parallel operation and calculate processing
message("Done.")
t[3] <- Sys.time()
print(paste("Serial processing time:",
    difftime(t[2], t[1], units = "secs"),
    "seconds"))
print(paste("Parallel processing time:",
    difftime(t[3], t[2], units = "secs"),
    "seconds"))

# Test to see of the two results are the same
print("Are the objects the same?")
identical(tumors_serial, tumors_parallel)

# Create a function to process from normalize to umap
process_seurat <- function(x) {
    x <- x %>%
        NormalizeData(verbose = FALSE) %>%
        FindVariableFeatures(verbose = FALSE) %>%
        ScaleData(verbose = FALSE) %>%
        RunPCA(verbose = FALSE) %>%
        FindNeighbors(verbose = FALSE) %>%
        FindClusters(verbose = FALSE) %>%
        RunUMAP(dims = 1:20, verbose = FALSE)
    return(x)
}

# Set the time for starting the serial processing
message("Starting serial processing of the Seurat objects...")
t[4] <- Sys.time()

# Map the function to the list of Seurat objects using lapply
tumors_serial <- lapply(tumors_serial, process_seurat)

# Mark completion of the previous operation and start of the next
message("Starting parallel processing of the Seurat objects...")
t[5] <- Sys.time()

# Map the function using mclapply
tumors_parallel <- mclapply(tumors_serial, process_seurat, mc.cores = 10L)

# Mark completion and calculate times
message("Done.")
t[6] <- Sys.time()
print(paste("Serial processing time:",
    difftime(t[5], t[4], units = "secs"),
    "seconds"))
print(paste("Parallel processing time:",
    difftime(t[6], t[5], units = "secs"),
    "seconds"))

# Are these two objects identical?
print("Are these two objects identical?")
identical(tumors_serial, tumors_parallel)

# Compare jobs run within a single operation
single_op <- function(x) {
    id <- x$id
    message(paste(id, "is starting..."))
    x <- create_seurat(x) %>%
        process_seurat()
    message(paste(id, "has completed."))
    return(x)
}

message("Starting the combined creating/processing approach...")
t[7] <- Sys.time()

tumors_parallel_2 <- mclapply(geo_data_2, single_op, mc.cores = 10L)

t[8] <- Sys.time()

print(paste("Total time for create, then process:",
    difftime(t[3], t[2], units = "secs") + difftime(t[6], t[5], units = "secs"),
    "seconds"))
print(paste("Total time for create + process in a single step:",
    difftime(t[8], t[7], units = "secs"),
    "seconds"))
```

## Practice 2: Be a good doobie (time permitting)

***A NOTE ABOUT COMPUTATIONAL STEWARDSHIP***
For the purposes of this course, we are performing parallel computing operations using an interactive session, to which we've assigned several computing resources. Generally, this is a very inefficient way to utilize resources, because you are really only using all of the requested CPUs for brief bursts of activity. The rest of the time, those cores sit idle while you write your code, but are not available for others to use. Leaving idle sessions like this running for long periods of time is poor form on a resource that is free to you (like Franklin) and a good way to spend a lot of money on computing power that you're not actually using. 

While interactive sessions like this can be helpful for development and debugging, they should generally be avoided. Also, when using an interactive session requesting multiple computing nodes, you should try to limit the number of nodes requested to those that you actually need and limit the time that you maintain the allocation (ie, close the session when you are done).

So, is there a way to be more efficient AND be a good citizen of our cyberspace?

YES! Here are some options:
1. Break your code down into sections that can be run as batch submissions through slurm.
2. Automate the above using the rslurm package. Run your interactive session on a single core (only requesting a single core in your srun or salloc interactive session scheduling request).
3. Find a middle ground. Requesting 3 cores will still speed up your data-intense operations about 3-fold, but leaves a lot more computing resources for others. (Or costs a lot less if you're paying for your wall hours.)
4. Make sure you turn things on and off to reduce your footprint. Use srun ... R to start your R sessions

Try one of these potential solutions above and see how it affects performace using benchmarks.

```{r parallel-slurm-r}
# Run the same block of code above as a slurm batch submission using rslurm
library(rslurm)

# Mark completion of the previous operation and start of the next
message("Starting parallel processing of the Seurat objects...")
t[9] <- Sys.time()

# Map the function using slurm_map
slurm_job <- slurm_map(tumors_serial,
    process_seurat,
    nodes = 1,
    cpus_per_node = length(tumors_serial))
tumors_parallel_3 <- get_slurm_out(slurm_job,
    outtype = "raw",
    wait = TRUE)

# Mark completion and calculate times
message("Done.")
t[10] <- Sys.time()
print(paste("Parallel processing time - mclapply:",
    difftime(t[5], t[4], units = "secs"),
    "seconds"))
print(paste("Parallel processing time - slurm_map:",
    difftime(t[6], t[5], units = "secs"),
    "seconds"))
```

## Discussion of pseudotime analysis/trajectory inference

This project gives a brief introduction to the world of pseudotime and trajectory methodologies. The large number of tools that have been developed to address this problem highlights the fact that this is a challenge with an array of imperfect solutions. Each approach provides solutions that address at least one type of problem, while no solution works well for every problem. Finding "true" trajectories often requires some trial and error using multiple different algorithms, which can often be complimentary.

A very nice comparative evaluation of the different approaches to pseudotime/trajectory analysis has been published by Saelens et al (https://doi.org/10.1038/s41587-019-0071-9). It really is worth a read if you want to understand more. You might also check out the metapackage the authors built to support their comparative benchmarking (https://github.com/dynverse/dyno). It's a little outdated at this point, but they did a good job putting it together.

Once weakness in nearly all of these approaches is that trajectories are almost entirely inferential. They should be viewed as different ways to "connect the dots," but they are NOT clear evidence that one cell type/cluster gives rise to another. Moreover, while some algorithms will generate educated guesses as to the directionality in the different connections between clusters, inferences of origin and destination are really just guesses. 

One can often use techniques such as RNA Velocity Analysis to get more direct evidence for directionality and evolutionary relationships. Indeed, these two techniques should be viewed as being highly complimentary. We will not have time to dive into velocity analysis today (maybe next year), but the following resources are a good place to start if you'd like to learn more:
 - https://doi.org/10.1371/journal.pcbi.1010492
 - https://doi.org/10.15252/msb.202110282
 - https://scvelo.readthedocs.io/en/stable/


## Session challenge

To take on today's challenge, install the dyno package* and then use it to compare results of several different approaches to trajectory inference using the same object(s). The data could be the same dataset used in class today, one taken from a repository, or one of your own. To qualify for judging, you must generate at least 8 distinct plots/analyses that evaluate different trajectories or gene expression modules that vary with pseudotime. Winners will be selected based on the following principles:
 - how appropriate the analyses are for addressing the problem/question
 - how creative the approach is
 - how well the approach is documented
 - how beautifully/accessibly the results are presented

To submit your code for judging, just save a new file titled "Session10-Challenge-Ryan_Roberts.Rmd" (replacing my name with yours, obviously) in the "Challenges" folder on the SCRGOT Coder Upgrade OneDrive.

*NOTE: Running the dyno package requires singularity. Singularity joined forces with the Linux foundation and became Apptainer, which is already installed on the Franklin cluster. When you run singularity commands, these will be automatically run by apptainer, and it works fine. However, if you run test_singularity_installation and use the "detailed = TRUE" option, it will tell you that the version is old. Just ignore this. Any apptainer installation is equivalent to a singularity version >3.8.

```{r}
system("ml load GCCcore/9.3.0 ImageMagick")
library(dyno)

# Set up the dyno object from the Seurat data
# First, add the raw and normalized counts
dyn_obj <- wrap_expression(
    expression = GetAssayData(seurat_obj, slot = "data", assay = "RNA") %>% t(),
    counts = GetAssayData(seurat_obj, slot = "counts", assay = "RNA") %>% t())

# Second, add the clustering information
dyn_obj <- add_grouping(dyn_obj, seurat_obj$assignment)
 
# And the UMAP embeddings
dyn_obj <- add_dimred(dyn_obj, Embeddings(seurat_obj[["umap"]]))

# And the starting cell
# Interactive steps run previously:
# CellSelector(DimPlot(seurat_obj))
dyn_obj <- add_prior_information(dyn_obj, 
    start_id = "OS_3_GTTGAACTCTATCGTT-1")

# Use the interactive shiny tool to explore the different methods and choose optimal
picked <- guidelines_shiny(dyn_obj)

# Run the trajectory inferrence algorithms

```


#########################################################################
## Section 2: Comparing two integration methods: Harmony and CCA.     
#########################################################################

# Introduction to compare current integration methods (10-15 min)

    -- Different integration method overview
    -- Factors to consider when chooseing an integration methods: project biological variations, batch effect.
    -- Visualization and intepretion of integrated data
    -- Assessment of integration quality
    -- Challenges and limitations


# Training session: this section will compare two widely used methods -- harmony and CCA. (20-30 min)

Method Description:
    -- Merge objects
    -- Run with harmony
    -- Run with CCA
    -- Visualie results from haramony and CCA

Dataset Description: this tutorial will have four samples, 
    -- two replicates from human normal kidney (GSM5627690, GSM5627691), 
    -- two replicates from  autosomal dominant polycystic kidney disease (GSM5627695 and GSM5627696).
    
Note: The dataset can be downloaded from GEO:GSE185948 : https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE185948 
Dataset reference: Muto Y, Dixon EE, Yoshimura Y, Wu H et al. Defining cellular complexity in human autosomal dominant polycystic kidney disease by multimodal single cell analysis. Nat Commun 2022 Oct 30;13(1):6497. PMID: 36310237
To reduce the time, We randomly select 1000 of cells in each sample. Please download the datasets from OneDrive.
https://nationwidechildrens-my.sharepoint.com/:u:/r/personal/katherine_miller_nationwidechildrens_org/Documents/SCRGOT/2023%20Coder%20Upgrade/Sessions/07-Integration/KidneyDatasets.zip?csf=1&web=1&e=cSDEwB


# Step 2.1: Load the packages and setting the working environment.

```{r setup, include=FALSE}
# Load the packages. 
# If you cannot load the packages, please download and install them: install.packages("harmony"); install.packages("gridExtra")
library(harmony)
library(Seurat)
library(gridExtra)
library(ggplot2)
library(tidyverse)

# Please remember to set up your work directory. 
# We set up the directory into folder where we downloaded the datasets
getwd() # if you want to get the current directory.
setwd("/Users/XXW004/Documents/Projects/LectureForSingleCellGroup/IntegrationSection/TestDatasets/") # If you want to set up the work diretory

# Check the files in the current directory. 
Datafiles<-list.files(path = "./", recursive = F, full.names = F)
Datafiles

```

# Step 2.2: Read the objects.

```{r}
# We used for loop to read the objects, add group column for each meta object, and save the object into a list accordingly to their names.

# Create an empty list to store the objects
myobjectlist<- list()

# Read the objects, assigan the name to the object, create a group to show the name of sample, and store these objects into a list

for (x in Datafiles){

  ## find the file with the substring of ".small.rds", then replace with empty string
  name <- gsub (".small.rds","", x)
  
  ## loading the RDS file
  rds<- readRDS(file = x)
  
  ## check the size of each objects
  dim(rds)
 
  ## using the assign the name to the objects
  assign (name, rds)
  
  ## create a group that show the name of samples
  rds$group <- rep(name, length(rds$orig.ident))
  
  ## store these objects into list
  myobjectlist[[name]] <-rds
}

```

# Step 2.3: Briefly go through these objects to confirm the lists: size of the list, meta data

```{r}
# check the lists. In the list, we will have four objects with their corresponding names
myobjectlist

# Check how many objects in the list
length(myobjectlist)

# Check the first object meta data
myobjectlist[[1]]@meta.data

# Step 2.4: Merge the objects
## Merge multiple objects from the list, add cell id with different prefix, and create a project for the merged object.
scrna<-merge(x=myobjectlist[[1]], 
    y=c(myobjectlist[[2]], myobjectlist[[3]],myobjectlist[[4]]), 
    add.cell.ids = c("A","B","C","D"), 
    project="Integration")

# Check the structure of meta data
str(scrna@meta.data)

# View the meta data
View(scrna@meta.data)

```

# Step 2.4: Quality control of merged objects

```{r}
## QC & filtering
## calculate mitochondrial percentatge
scrna$mitoPercent <-PercentageFeatureSet(scrna, pattern = '^MT-')

# let's check the quality of datasets by eveluating the mitochondrial percentage, number of Counts, number of features.
head(scrna@meta.data)
VlnPlot(scrna, features = c("mitoPercent", "nCount_RNA", "nFeature_RNA"))
VlnPlot(scrna, features = c("mitoPercent", "nCount_RNA", "nFeature_RNA") , split.by = 'group')

# filtering
scrna <- subset (scrna, subset =mitoPercent <10 & nFeature_RNA >500 & nCount_RNA >200 )

```

# Step 2.5: Standard normalization, variable feature finding, PCA and UMAP analyses, and Plot the merged objects

```{r}

# perform the standard workflow to figure out if there are any batch effects
scrna<- NormalizeData(object = scrna)
scrna<- FindVariableFeatures(object = scrna, nfeatures = 3000)
scrna<- ScaleData(object = scrna)
scrna<- RunPCA(object = scrna, npcs =15)
scrna<- RunUMAP(scrna, dims = 1:15)

# Visualize the dimplot by the merge function
BeforeHarmony<- DimPlot(scrna, reduction = "umap",split.by = 'group')
BeforeHarmony

```

# Step 2.6: Run with the harmony 

```{r}

# Before running Harmony, we need to confirm the metadata of our Seurat object contains one or several variables describing the factors we want to integrate on.
# We have defined the variable "group" to distinguish different samples.
# Double check:
levels(factor(scrna@meta.data$group))
scrna@meta.data

# note: we add a new reduction of 15 harmony 
seurat.harmony.integrated <- RunHarmony(scrna, group.by.vars = 'group', dims.use = 1:15, plot_convergence= FALSE, project.dim = F)

# Check the reduction after running harmony
seurat.harmony.integrated@reductions
seurat.harmony.integrated.embed <- Embeddings(seurat.harmony.integrated, "harmony")
seurat.harmony.integrated.embed[1:10,1:10]

# Check whether harmony integration is reflected in the visualization, we also generate UMAP using harmony reduction.
# Run Umap and clustering using Harmony reduction

seurat.harmony.integrated <- RunUMAP(seurat.harmony.integrated,reduction='harmony', dim=1:15)

# Run the cluster also need to set up the harmony reduction to find neighbor and clusters.
seurat.harmony.integrated<-FindNeighbors(seurat.harmony.integrated, reduction = 'harmony', dims = 1:15)
seurat.harmony.integrated<- FindClusters(seurat.harmony.integrated, resolution=1)


# Visualization of the dimplot using the harmony 
DimHarmony<-DimPlot(seurat.harmony.integrated, reduction = 'umap', split.by = 'group')
DimHarmony

```

# Step 2.7: Run with the canonical correlation analysis (CCA)

# Let's compare between CCA and Harmony.. 

```{r}
# Find integration anchor (CCA)
# for four split objects to perform the CCA method
# As we have already performed the QC, we used the objects listed above.

# Split the objects based on the group
SplitedObjects<- SplitObject(scrna, split.by = 'group')

# Check the split objects
SplitedObjects
length(SplitedObjects)

# Normalized dataset and Find variable features
for (i in 1: length(SplitedObjects)){
  SplitedObjects[[i]] <-NormalizeData(object = SplitedObjects[[i]])
  SplitedObjects[[i]] <- FindVariableFeatures(object = SplitedObjects[[i]],selection.method = "vst")
}

# Select integration features
features<- SelectIntegrationFeatures(object.list = SplitedObjects)
head(features)

# Find the integration anchor (CCA)
anchors<- FindIntegrationAnchors(object.list = SplitedObjects, anchor.features = features)

# Integrate data. This might take a longer time ...
seurat.integrated <- IntegrateData(anchorset = anchors)

# Scale data, run PCA and UMAP and visualize integrated data
seurat.integrated <- ScaleData(object = seurat.integrated)
seurat.integrated <- RunPCA(object = seurat.integrated)

# We can see from the PCA that a good overlay of several condtions by PCA
seurat.integrated<- FindNeighbors(seurat.integrated, dims = 1:15, reduction = "pca")
seurat.integrated <- FindClusters(seurat.integrated)
# Now, we can also visualize with UMAP.

seurat.integrated <- RunUMAP(object = seurat.integrated, dims = 1:15, reduction = "pca")

# Plot UMAP
DimPlot(seurat.integrated)

DimCCA <- DimPlot(seurat.integrated, reduction = 'umap', split.by  = 'group')
DimCCA

```

# Step 2.8: Compare two methods

```{r}

# Plot UMAPs from Harmony and CCA.
grid.arrange(DimHarmony, DimCCA,  ncol = 1, nrow=2)

# Potential way to evaluate the integration methods:
# Use the biomarkers from endothelium cells
FeaturePlot(seurat.harmony.integrated, features = c("PECAM1"), split.by = 'group', min.cutoff = 0.1)
FeaturePlot(seurat.integrated, features = c("PECAM1"), split.by = 'group', min.cutoff = 0.1)

```

## Break: 10 min

## Activity (20 min): Run the harmony with tumor datasets, and visualize the differences among harmony, merge and CCA.




